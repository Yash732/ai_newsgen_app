import yfinance as yf
from colorama import Fore
from mcp.server.fastmcp import FastMCP
import os
from supabase import create_client, Client
from smolagents import ToolCallingAgent, ToolCollection, LiteLLMModel
from litellm import completion
from typing import List
from dotenv import load_dotenv
import time
import requests
# from sentence_transformers import SentenceTransformer
load_dotenv()
##Using openAI model
from ai_resources.initialize_llm import chat_with_model
from ai_resources.initialize_db import get_embeddings, insert_embeddings

# mcp connection
mcp = FastMCP(
    "mcp-server",
    # for local testing, you can use "
    host = "127.0.0.1",
    # host= "0.0.0.0" ,
    port = 8000,
    )

#connection to supabase server
url: str = os.getenv("SUPABASE_URL")
key: str = os.getenv("SUPABASE_KEY")
supabase: Client = create_client(url, key)

# #initializing llm model
# model = LiteLLMModel(
#     model_id = "ollama/phi3"
# )

@mcp.tool()
def stock_info(stock_ticker: str) -> str:
    """
    Returns background company information for the given stock ticker symbol (e.g., 'AAPL', 'GOOG').

    Use this tool when the user requests stock details or profile information.

    Args:
        stock_ticker: A valid alphanumeric stock ticker (e.g., 'AAPL')
    """
    
    print("Our current stock: ",stock_ticker)
    dat = yf.Ticker(stock_ticker)
    return str(f"Background information for {stock_ticker}: {dat.info}")

@mcp.tool()
def stock_price(stock_ticker: str)->str:
    """
    Returns latest stock prices for the given stock ticker symbol (e.g., 'AAPL', 'GOOG').

    Use this tool when the user requests current stock prices or market data.

    Args:
        stock_ticker: A valid alphanumeric stock ticker (e.g., 'AAPL')
    """
    print("Our current stock: ",stock_ticker)
    dat = yf.Ticker(stock_ticker)
    historical_prices = dat.history(period = "1mo")

    last_months_close = historical_prices['Close']
    print(Fore.YELLOW + str(last_months_close))

    return str(f"Stock price over the last month for {stock_ticker}: {last_months_close}")

# llm model for summarization
# def summarize_news(news_data: List[str])-> str:
#     combined_text = "\n\n".join(news_data)

#     prompt = (
#         f"Summarize the following news, article by article. "
#         f"Only include relevant and factual information. Remove any ad content or links.\n\n{combined_text}"
#     )
#     print("********Starting summary generation through LLM**********")
#     ## liteLLM model
#     # response = completion(
#     #         model="ollama/phi3",
#     #     messages=[
#     #         {"role": "user", "content": prompt}
#     #     ],
#     #     api_base="http://localhost:11434"
#     #         )
#     # content = response.choices[0].message.content
#     start_time = time.time()
    
    
#     response = chat_with_model(prompt)
#     end_time = time.time()
#     elapsed_time = end_time - start_time
#     print(f"********Summary generated inside function in {elapsed_time}**********")
#     #response being a list or a string
#     content = response[0] if isinstance(response, list) else response
#     return content

@mcp.tool()
def latest_news(user_query: str)->str:
    """Fetches *real-time latest news* for the user's query using Tavily.
    Use this tool when the user asks for today's news or most recent updates.
    """
    #Utilizing Tavily API for user search
    tavily_api_key = os.getenv("TAVILY_API_KEY")

    query = f"{user_query}"
    url = "https://api.tavily.com/search"
    headers = {"Authorization": f"Bearer {tavily_api_key}"}
    max_results = 5 

    payload = {
        "query": query,
        "search_depth": "basic",   # "advanced" for more accurate results
        "include_answer": True,        # Include a concise summary
        "include_raw_content": False, # Set to True to include full page text
        "max_results": max_results
    }
    news_data = []
    response = requests.post(url, json=payload, headers=headers)

    if response.status_code == 200:
        data = response.json()
        results = data.get("results", [])

        if not results:
            return f"No news found."

        news_data = []
        for res in results:
            title = res.get("title", "No Title")
            snippet = res.get("content", "No summary available")
            url = res.get("url", "") #for appending the links as well
            news_data.append(f"* {title}\n{snippet}\n")
        #Saving the news data to the database through embeddings
        # print("***********Inserting news data into the database**********")
        # insert_embeddings(news_data)
        print("******raw news generated by tavily*******")

        return f"Latest news:\n\n{news_data}"
    else:
        return f"Tavily request failed: {response.status_code}"



# open-source embedding model
# model = SentenceTransformer('all-MiniLM-L6-v2')

# @mcp.resource("similar_news://search/{query}")
@mcp.tool()
def list_similar_articles(user_query: str) ->str:
    """
    Retrieves archived or semantically similar past news articles from Supabase.
    Not meant for fetching today's breaking news.
    """
    try:
        embedding = get_embeddings([user_query])[0] # To get the actual embedding vector from the list
        response = supabase.rpc("match_docs",{
            "query_embedding": embedding,
            "match_threshold": 0.78,
            "match_count":5
        }).execute()
        documents = response.data
        similar_news = []
        if documents:
            for doc in documents:
                similar_news.append(f"{doc['content']}\n")
            return str(similar_news)
        else:
            return f"No similar news related to your query"
    except Exception as e:
        return f"Error querying Supabase: {str(e)}"

if __name__ == "__main__":
    mcp.run(transport = "streamable-http")

