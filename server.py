import yfinance as yf
from colorama import Fore
from mcp.server.fastmcp import FastMCP
import os
from supabase import create_client, Client
from smolagents import ToolCallingAgent, ToolCollection, LiteLLMModel
from litellm import completion
from typing import List
from dotenv import load_dotenv
import requests
from sentence_transformers import SentenceTransformer
load_dotenv()
##Using openAI model
from ai_resources.initialize_llm import chat_with_model
from ai_resources.initialize_db import get_embeddings


# mcp connection
mcp = FastMCP(
    "yfin-server",
    host = "0.0.0.0", 
    port = 8000,
    request_timeout = 1000000
    )
# #initializing llm model
# model = LiteLLMModel(
#     model_id = "ollama/phi3"
# )



@mcp.tool()
def stock_info(stock_ticker: str)->str:
    """This tool returns information about a given stock given it's ticker.
    Args:
        stock_ticker: a alphanumeric stock ticker
        Example payload: "IBM"

    Returns:
        str:information about the company
        Example Respnse "Background information for IBM: {'address1': 'One New Orchard Road', 'city': 'Armonk', 'state': 'NY', 'zip': '10504', 'country': 'United States', 'phone': '914 499 1900', 'website': 
                'https://www.ibm.com', 'industry': 'Information Technology Services',... }" 
        """
    dat = yf.Ticker(stock_ticker)
    return str(f"Background information for {stock_ticker}: {dat.info}")

@mcp.tool()
def stock_price(stock_ticker: str)->str:
    """This tool returns the last known price for a given stock ticker.
    Args:
        stock_ticker: a alphanumeric stock ticker 
        Example payload: "NVDA"

    Returns:
        str:"Ticker: Last Price" 
        Example Respnse "NVDA: $100.21" 
        """
    dat = yf.Ticker(stock_ticker)
    historical_prices = dat.history(period = "1mo")

    last_months_close = historical_prices['Close']
    print(Fore.YELLOW + str(last_months_close))

    return str(f"Stock price over the last month for {stock_ticker}: {last_months_close}")


import time

# llm model for summarization
def summarize(news_data: List[str])-> str:
    print(f"****\n {type(news_data)} \n****")
    combined_text = "\n\n".join(news_data)

    prompt = (
        f"Summarize the following news, article by article. "
        f"Only include relevant and factual information. Remove any ad content or links.\n\n{combined_text}"
    )
    print("********Starting summary generation through LLM**********")
    ## liteLLM model
    # response = completion(
    #         model="ollama/phi3",
    #     messages=[
    #         {"role": "user", "content": prompt}
    #     ],
    #     api_base="http://localhost:11434"
    #         )
    # content = response.choices[0].message.content
    start_time = time.time()
    
    
    response = chat_with_model(prompt)
    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f"********Summary generated inside function in {elapsed_time}**********")
    #response being a list or a string
    content = response[0] if isinstance(response, list) else response
    print(type(content))
    return content


#Utilizing Tavily API for user search
tavily_api_key = os.getenv("TAVILY_API_KEY")
@mcp.tool()
def stock_news(stock_ticker: str)->str:
    """This tool returns the news data for the stock company whose stock ticke is provided by the user from tavily search.
    Args:
        stock_ticker: a alphanumeric stock ticker 
        Example payload: "NVDA"

    Returns:
        List[str]: "Ticker: News data" 
        Example Response:
        "
        News1 data
        News2 data
        ...
        " 
    """
    query = f"{stock_ticker} stock news"
    url = "https://api.tavily.com/search"
    headers = {"Authorization": f"Bearer {tavily_api_key}"}
    max_results = 5 

    payload = {
        "query": query,
        "search_depth": "advanced",   # "basic" for faster results
        "include_answer": True,        # Include a concise summary
        "include_raw_content": False, # Set to True to include full page text
        "max_results": max_results
    }
    news_data = []
    response = requests.post(url, json=payload, headers=headers)

    if response.status_code == 200:
        data = response.json()
        results = data.get("results", [])

        if not results:
            return f"No news found for {stock_ticker}."

        news_data = []
        for res in results:
            title = res.get("title", "No Title")
            snippet = res.get("content", "No summary available")
            url = res.get("url", "") #for appending the links as well
            news_data.append(f"* {title}\n{snippet}\n")
        print("******raw news generated by tavily*******")
        summarized_news = summarize(news_data)
        print("************returning final summarized news ********")
        return f"Important news for {stock_ticker}:\n\n{type(summarized_news)}"

    else:
        return f"Tavily request failed: {response.status_code}"

#connection to supabase server
url: str = os.getenv("SUPABASE_URL")
key: str = os.getenv("SUPABASE_KEY")
supabase: Client = create_client(url, key)
model = SentenceTransformer('all-MiniLM-L6-v2')

# @mcp.resource("similar_news://search/{query}")
@mcp.tool()
def list_similar_articles(query: str) ->str:
    """
    This resource allows you to fetch similar news by passing a query,
    returning similar news from Supabase using pgvector semantic search.
    """
    try:
        embedding = model.encode(query).tolist()
        response = supabase.rpc("match_documents",{
            "query_embedding": embedding,
            "match_threshold": 0.78,
            "match_count":5
        }).execute()
        documents = response.data
        similar_news = []
        if documents:
            for doc in documents:
                similar_news.append(f"{doc['content']}\n")
            summarized_similar_news = summarize(similar_news)
            return summarized_similar_news
        else:
            return f"No similar news related to your query"
    except Exception as e:
        return f"Error querying Supabase: {str(e)}"

# response = list_similar_articles("Elon Musk xAI")
# print(response)
if __name__ == "__main__":
    mcp.run(transport = "streamable-http")
